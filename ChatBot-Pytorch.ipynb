{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext.data as data\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "question = data.Field(lower=True, \n",
    "                      init_token=\"<sos>\",\n",
    "                      eos_token=\"<eos>\",\n",
    "                      tokenize=tokenizer)\n",
    "\n",
    "answer = data.Field(lower=True, \n",
    "                    init_token=\"<sos>\", \n",
    "                    eos_token=\"<eos>\", \n",
    "                    tokenize=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = {\"Question\":(\"q\", question), \"Answer\":(\"a\", answer)}\n",
    "dataset = data.TabularDataset(path=\"questions_answers.csv\", format=\"csv\", fields=datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question.build_vocab(dataset, min_freq=2)\n",
    "answer.build_vocab(dataset, min_freq=2)\n",
    "train_data, valid_data = dataset.split(split_ratio=0.7)\n",
    "train_iter = data.BucketIterator(\n",
    "        train_data,\n",
    "        batch_size=32,\n",
    "        device='cuda',\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.q)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedding_size, \n",
    "                 src_vocab_size,\n",
    "                 trg_vocab_size,\n",
    "                 src_pad_index,\n",
    "                 num_heads,\n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 dense_dim,\n",
    "                 dropout,\n",
    "                 max_len,\n",
    "                ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            dense_dim,\n",
    "            dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_index = src_pad_index\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_index\n",
    "        return src_mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_seq_len, N = src.shape\n",
    "        trg_seq_len, N = trg.shape\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_len).unsqueeze(1).expand(src_seq_len, N).cuda()\n",
    "        )\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_len).unsqueeze(1).expand(trg_seq_len, N).cuda()\n",
    "        )\n",
    "        embed_src = self.dropout((self.src_word_embedding(src) + self.src_position_embedding(src_positions)))\n",
    "        embed_trg = self.dropout((self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions)))\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_len).cuda()\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask = src_padding_mask,\n",
    "            tgt_mask=trg_mask\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 125\n",
    "learning_rate = 3e-4\n",
    "src_vocab_size = len(question.vocab)\n",
    "trg_vocab_size= len(answer.vocab)\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.1\n",
    "max_len = max(max([batch.a.shape[0] for batch in train_iter]), max([batch.q.shape[0] for batch in train_iter]))\n",
    "dense_dim = 64\n",
    "src_pad_index = question.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "net = Transformer(embedding_size,  src_vocab_size, \n",
    "                  trg_vocab_size, src_pad_index, \n",
    "                  num_heads, num_encoder_layers,\n",
    "                  num_decoder_layers, dense_dim, dropout, max_len).cuda()\n",
    "\n",
    "trg_pad_index = answer.vocab.stoi[\"<pad>\"]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=trg_pad_index)\n",
    "opt = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt, factor=0.1, patience=10, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Who suggested Lincoln grow a beard?\" \n",
    "def predict(test_sentence, model):\n",
    "    max_len = 100\n",
    "    sen = [w.text.lower() for w in spacy_en.tokenizer(test_sentence)]\n",
    "    sen.insert(0, question.init_token)\n",
    "    sen.append(question.eos_token)\n",
    "    inp_sen = [question.vocab.stoi[i] for i in sen]\n",
    "    inp_sen = torch.tensor(inp_sen, dtype=torch.long).unsqueeze(1).cuda()\n",
    "    outputs = [answer.vocab.stoi[\"<sos>\"]]\n",
    "    for i in range(max_len):\n",
    "        trg = torch.tensor(outputs, dtype=torch.long).unsqueeze(1).cuda()\n",
    "        with torch.no_grad():\n",
    "            output = model(inp_sen, trg)\n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        outputs.append(best_guess)\n",
    "        if best_guess == answer.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "    pred_sentence = [answer.vocab.itos[i] for i in outputs]\n",
    "    return pred_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> least too 10 blaine too do months months months do least too 10 being being being months do ursidae : months months buffalo too least blanco formally simple stronghold being monroe kivi blanco months across do france being monroe recognized monroe if mining buffalo months blaine do ursidae blaine do ursidae too 10 recognized monroe months cello recognized monroe buffalo blaine do ursidae : least too 10 months america pascal monroe monroe members least kilometers monroe pull entries blaine do least ursidae simple least kilometers months gouverneur least too cello recognized do months high if mining : formally r. recognized\n",
      "[Epoch 0] Loss 4.906983041763306\n",
      "<sos> <unk> <eos>\n",
      "[Epoch 1] Loss 4.339334112803141\n",
      "<sos> the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "[Epoch 2] Loss 3.9519260629018147\n",
      "<sos> a <eos>\n",
      "[Epoch 3] Loss 3.551300090154012\n",
      "<sos> john <unk> <eos>\n",
      "[Epoch 4] Loss 3.1755897426605224\n",
      "<sos> grace tesla <eos>\n",
      "[Epoch 5] Loss 2.80565261999766\n",
      "<sos> grace octopus has a <unk> . <eos>\n",
      "[Epoch 6] Loss 2.4659778356552122\n",
      "<sos> grace <unk> <eos>\n",
      "[Epoch 7] Loss 2.1444581921895347\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 8] Loss 1.8431393361091615\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 9] Loss 1.595298992395401\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 10] Loss 1.3281395363807678\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 11] Loss 1.1374501184622448\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 12] Loss 1.0026607267061869\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 13] Loss 0.8564311148722966\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 14] Loss 0.7440702426433563\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 15] Loss 0.6346450215578079\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 16] Loss 0.574513382713\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 17] Loss 0.5163803281386693\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 18] Loss 0.47771706541379294\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 19] Loss 0.43995963245630265\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 20] Loss 0.4053815390666326\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 21] Loss 0.3716909963885943\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 22] Loss 0.35555777450402576\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 23] Loss 0.3316009611884753\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 24] Loss 0.31792183478673297\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 25] Loss 0.29236794461806614\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 26] Loss 0.29443570201595626\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 27] Loss 0.2788750368356705\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 28] Loss 0.26626162221034366\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 29] Loss 0.25855111454923946\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 30] Loss 0.2525020055472851\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 31] Loss 0.24815799171725908\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 32] Loss 0.24333148444692293\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 33] Loss 0.23253917862971624\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 34] Loss 0.23204803504049779\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 35] Loss 0.22443684379259746\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 36] Loss 0.22205116962393126\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 37] Loss 0.2146360661337773\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 38] Loss 0.2230271671215693\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 39] Loss 0.20994978892306487\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 40] Loss 0.2026816272487243\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 41] Loss 0.21192966418961684\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 42] Loss 0.19274244492252668\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 43] Loss 0.1924369159837564\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 44] Loss 0.1976955593874057\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 45] Loss 0.18036225569744904\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 46] Loss 0.1970039855192105\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 47] Loss 0.18298043022553126\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 48] Loss 0.17617300420999527\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 49] Loss 0.17806182893613975\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 50] Loss 0.18699497406681378\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 51] Loss 0.17570700959612925\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 52] Loss 0.1843920046215256\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 53] Loss 0.16990490898489952\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 54] Loss 0.16455421256522337\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 55] Loss 0.17145465798676013\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 56] Loss 0.1695789703230063\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 57] Loss 0.16918681473781666\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 58] Loss 0.16793163560330868\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 59] Loss 0.1733487705886364\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 60] Loss 0.1612367509305477\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 61] Loss 0.165561420917511\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 62] Loss 0.16072587283949058\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 63] Loss 0.15637879145642122\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 64] Loss 0.15544670220464468\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 65] Loss 0.1543206090728442\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 66] Loss 0.1536894260098537\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 67] Loss 0.14896874936918417\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 68] Loss 0.15030603980024657\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 69] Loss 0.15869373413423696\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 70] Loss 0.14232258463899294\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 71] Loss 0.14345912086466947\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 72] Loss 0.1477324734007319\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 73] Loss 0.1475649449725946\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 74] Loss 0.143578448711584\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 75] Loss 0.14502718115846316\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 76] Loss 0.1397376246812443\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 77] Loss 0.13866871739427247\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 78] Loss 0.13427208763857681\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 79] Loss 0.1465287610143423\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 80] Loss 0.14333275289585193\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 81] Loss 0.1383185505742828\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 82] Loss 0.13845640667403739\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 83] Loss 0.12876441719631354\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 84] Loss 0.13633871786296367\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 85] Loss 0.14030026756227015\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 86] Loss 0.13572280400743086\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 87] Loss 0.1385398903489113\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 88] Loss 0.13787647149215143\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 89] Loss 0.1366490299999714\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 90] Loss 0.12918948203325273\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 91] Loss 0.1264197042149802\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 92] Loss 0.1298561727628112\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 93] Loss 0.13107061648120483\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 94] Loss 0.13424221840376654\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 95] Loss 0.12804044932126998\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 96] Loss 0.13206949474910895\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 97] Loss 0.1289108002682527\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 98] Loss 0.12763391062617302\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 99] Loss 0.12961231822768848\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 100] Loss 0.12442214200894038\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 101] Loss 0.12420344764987627\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 102] Loss 0.12561647849778335\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 103] Loss 0.1254134823133548\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 104] Loss 0.1215574794759353\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 105] Loss 0.11742412716150284\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 106] Loss 0.11979232892394066\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 107] Loss 0.1254409493257602\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 108] Loss 0.12077710838367542\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 109] Loss 0.11981632160643736\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 110] Loss 0.12312667306512594\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 111] Loss 0.11823132701218128\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 112] Loss 0.12240158312022686\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 113] Loss 0.12452817658583323\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 114] Loss 0.11793508919576803\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 115] Loss 0.11682884529232979\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 116] Loss 0.11600925685216983\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 117] Loss 0.11788715366274119\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 118] Loss 0.11343174602215489\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 119] Loss 0.11735617510974407\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 120] Loss 0.1139012940103809\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 121] Loss 0.11671146304657062\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 122] Loss 0.1103136146441102\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 123] Loss 0.1195490238815546\n",
      "<sos> grace bedell . <eos>\n",
      "[Epoch 124] Loss 0.11245346163709959\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    net.eval()\n",
    "    pred_sen = predict(test_sentence, net)\n",
    "    print(\" \".join(pred_sen))\n",
    "    losses = []\n",
    "    for data in train_iter:\n",
    "        net.train()\n",
    "        outs = net(data.q, data.a[:-1, :])\n",
    "        outs = outs.reshape(-1, outs.shape[2])\n",
    "        target = data.a[1:].reshape(-1)\n",
    "        opt.zero_grad()\n",
    "        loss = loss_fn(outs, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "    print(f\"[Epoch {epoch}] Loss {mean_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'grace', 'bedell', '.', '<eos>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_net = torch.load(\"chatbot_model.pth\").cuda()\n",
    "predict(test_sentence, new_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(vocab, path):\n",
    "    with open(path, 'w+', encoding='utf-8') as f:     \n",
    "        for token, index in vocab.stoi.items():\n",
    "            f.write(f'{index}\\t{token}')\n",
    "\n",
    "# save_vocab(question.vocab, \"src_vocab.txt\")\n",
    "# save_vocab(answer.vocab, \"trg_vocab.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
